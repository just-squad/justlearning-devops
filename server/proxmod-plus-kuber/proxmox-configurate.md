# Настройка Proxmox

Proxmox позволяет производить настройку с Web UI, но настройки достаточно ограниченные, поэтому в большинстве случаев мы будем использовать подключение по `ssh` под `root` пользователем (никто не запрещает создать своего пользователя и дать ему необходимые права).

При входе в Proxmox есть 2 типа аутентификации

![img_1](./images/image-2.png)

**Linux PAM standard authentication** - аутентификация на основе пользователя именно системы Linux. Для добавления нового пользователя, который сможет проходить такую аутентификацию нужно сначала создать пользователя через консоль, а потом добавить пользователя с тем же логином в UI Proxmox

**Proxmox VE authentication server** - аутентификация непосредственно в системе Proxmox UI. Этот пользователь никак не связан с ОС Linux и существует только в системе Proxmox. Отлично подходит если вы ведете настройки только через UI. В консоли или по SSH таким под таким пользователем залогиниться не получится. Добавить такого пользователя можно просто через UI.

![img_2](./images/image-3.png)

Желательно, после добавления дополнительного пользователя для аутентификации в UI, отключить аутентификацию пользователя `root` и включить двухфакторную аутентификацию. Например по одноразовым ключам `TOTP`

Для удобства подключения по SSH лучше сгенерировать ключ и прописать его на сервере. Почитать можно например [тут](https://www.cyberciti.biz/faq/how-to-set-up-ssh-keys-on-linux-unix/)

Так же подключиться к консоли можно и через WebUi

![img_3](./images/image-4.png)

## Редактирование сетевых интерфейсов

Используя nano или другой редактор заходим в `/etc/network/interfaces` и поменяем интерфейсы:

![img_4](./images/image-5.png)

здесь уже были 2 интерфейса – `enp3s0` и `vmbr0`

**enp3s0** – это наше физическое подключение по кабелю
**vmbr0** – это bridge этого подключения

Приведем данные домашней сети для примера:

**Gateway (роутер):** 192.168.50.1

**Host (машина с Proxmox):** 192.168.50.252

``` text
# network interface settings; autogenerated
# Please do NOT modify this file directly, unless you know what
# you're doing.
#
# If you want to manage parts of the network configuration manually,
# please utilize the 'source' or 'source-directory' directives to do
# so.
# PVE will preserve these directives, but will NOT read its network
# configuration from sourced files, so do not attempt to move any of
# the PVE managed interfaces into external files!

auto lo
iface lo inet loopback

auto enp3s0
iface enp3s0 inet manual

auto vmbr0
iface vmbr0 inet static
        address 192.168.50.252/24
        gateway 192.168.50.1
        bridge-ports enp3s0
        bridge-stp off
        bridge-fd 0

# Dedicated internal network for Kubernetes cluster
auto vmbr1
iface vmbr1 inet static
        address 10.0.1.1
        netmask 255.255.255.0
    bridge-ports none
    bridge-stp off
    bridge-fd 0
    post-up   echo 1 > /proc/sys/net/ipv4/ip_forward
    post-up   iptables -t nat -A POSTROUTING -s '10.0.1.0/24' -o vmbr0 -j MASQUERADE
    post-down iptables -t nat -D POSTROUTING -s '10.0.1.0/24' -o vmbr0 -j MASQUERADE

iface wlp2s0 inet manual

source /etc/network/interfaces.d/*
```

Для сервера с Proxmox зададим статический IP `192.168.50.252` и пропишем его на физическом роутере чтобы DHCP не менял его.

Далее нужно создать внутреннюю сеть для наших K8s машин – для этого создается отдельный интерфейс `vmbr1`.

Прописываем ему статический IP `10.0.1.1`. Так как Proxmox будет нашим Gateway для внутренних машин, а также нашим DHCP и DNS сервером для внутренней сети (при желании DHCP и DNS серверы можно выделить на отдельные машины или контейнеры внутри этой сети – но для упрощения эти сервера будут на самом хосте Proxmox)

Далее идет блок с post-up где мы прописываем правила маршрутизации iptables для того чтобы машины внутри нашей сети могли иметь доступ во внешнюю сеть и интернет.

В итоге наша сеть должна выглядеть вот так:

![img_5](./images/image-6.png)

Все k8s машины должны быть во внутренней сети 10.0.1.1/24 с доступом во внешнюю сеть, но без доступа из внешней сети напрямую.

Так же у нас будет отдельная машина Bastion которая будет зарегистрирована в обоих сетях. Через нее будет доступ к внутренней сети, на ней так же будет Reverse-proxy который будет перенаправлять внешние запросы на k8s.

Но для того чтобы у нас были машины – там нужно их создать.

Можно выбрать любой дистрибутив Linux, который вам нравится. И создать виртуальную машину на основе выбранного дистрибутива. Но лучше сначала создать "шаблон" виртуальной машины, чтобы потом проще было создавать новые экземпляры, просто копируя шаблон.

Для создания шаблона, можно почитать [вот этот раздел](./proxmox-create-vm-template.md)




Настроим Сеть:

Захоидм на машину через UI – и устанавливаем /etc/hostname на необходимый нам
Все наши машины которые будут находится во внутренней сети будут иметь домен k8s.internal так что необходимый нам hostname в данном случае будет node1

Таким образом все ноды k8s будут доступны по node{n}.k8s.internal а контроллер – controller.k8s.internal, так же bastion будет доступен во внешней сети по bastion.k8s.internal

Заходим в /etc/network/interfaces на этой машине и меняем настройку сети

Ставим интерфейсу соединения получение IP по dhcp – сейчас нам это никак не поможет, но вдальнейшем – понадобится

Так же доставляем необходимый пакет ssh

Теперь нам нужно сделать машину для нашего Bastion

Копируем получившуюся машину

Заходим на нее – меняем соответственно /etc/hostname

и меняем настройки сети

В данном случае Bastion будет присутствовать в нескольких сетях – то есть в нашей локальной (192.168.50.0) и в нашей внутренней (10.0.1.0)
В случае если мы хотим добавить еще 1 контур – то есть например разделить на dev, stage, prod – то данная машина будет находится во всех внутренних сетях

Для bastion я прописал на роутере статический адрес - поэтому ей всегда будет выделятся только он – поэтому ens18 – можно оставить dhcp
Наша внутренняя сеть ens19 будет иметь свой dhcp – где мы сделаем тоже самое (пропишем статический IP) так что тоже можно оставить dhcp.

Далее нам необходимо настроить сами DHCP и DNS серверы

Для этого будем использовать isc-dhcp-server для DHCP и bind9 для DNS

Выходим с VM и заходим на основной proxmox сервер

Устанавливаем указаные выше пакеты и вдабавок nslookup и dig

После установки они попытаются запустится – но с дефолтной конфигурацией это не получится и они упадут с ошибкой – пока что это норм

Настройка DHCP:

Для начала нам нужно разметить нашу внутреннюю сеть – чтобы всем VM в сети 10.0.1.0 автоматически раздавались IP в диапазоне 10 – 200
10.0.1.1 – у нас занят самим PROXMOX
10.0.1.2 – у нас будет занят статическим IP VM Bastion
Диапазоны 10.0.1.3 – 10.0.1.9 и 10.0.1.201 – 10.0.1.254 выделяем под возможные дополнительные необходимые сервисы требующие статических IP

Заходим в /etc/dhcp/dhcpd.conf и меняем строчку ddns-update-style на
ddns-update-style interim;

и добавляем строчки
ignore client-updates;
update-static-leases on;
ddns-updates on;

Так как это наш Primary DHCP сервер ставим так же

authoritative;

и прописываем диапазон адресов в которых наш DHCP сервер будет раздавать IP а так же наши статические IP (диапазон выдаваемых IP и статические IP не должны пересекатся)

Обращаю внимание что все доменные имена прописаны с “.” вконце

Статические IP прописаны с привязкой к MAC адресу машины (посмотреть можно через “ip a” на конкретной машине или на интерфейсе proxmox)

Далее нужно прописать на каком интерфейсе наш DHCP будет выдавать IP – для этого заходим в /etc/default/isc-dhcp-server и прописываем наш интерфейс внутренней сети

На этом этапе – если все что нужно это DHCP – то можно удалить из /etc/dhcp/dhcpd.conf строки с ddns и domain-name и поменять ddns-update-style на none и все уже должно работать (учитывайте что IP получается при запуске машины или при перезагрузке network интерфейса), если нет – то подробности/ошибки/лог можно посмотреть в syslog

Настройка DNS:

При установке bind9 создается папка /etc/bind в которой генерятся дефолтные конфигурации и созданый при установке rndc.key (который можно перегенерить или сгенерировать еше 1 – подробнее об этом в документации к bind9)

Заходим в /etc/bind/named.conf и добавляем строчку
include "/etc/bind/rndc.key";
с тем ключем что был сгенерирован
а так же сверху, всей конфигурации, строчку
acl internals { 127.0.0.0/8; 10.0.1.0/24; };

Следует добавить – по комментарию сверху файла – разработчик рекомендует всю кастомную конфигурацию указывать в named.conf.local файле – однако почему то если указать зоны в named.conf.local файле они воспринимаются как часть options {…} секции и конфигурация падает – поэтому все зоны у меня описываются прямо в named.conf файле

здесь прописаны прямая и обратная зоны – нужно заметить что обратная зона имеет суффикс .in-addr.arpa что критично для объявления обратной зоны

в моем случае так как прямая зона это 10.0.1.0 то обратная это 1.0.10.in-addr.arpa

так же в конфигурации указаны файлы этих зон – а именно db-k8s.internal и db-1.0.10 (названия файлов могут быть какими угодно – главное чтоб вам было понятно) – эти файлы надо создать
Директория в которой bind будет искать файлы зон указана в файле named.conf.options

Содаем db-k8s.internal и db-1.0.10 в указаной выше папке

выглядит он примерно так

Следует учитывать что bind кеширует конфигурацию по ее serial так что если нужно что то изменить – то serial нужно инкрементировать – best practice в этом плане это использовать формулу YYYYMMDDNN – где NN это номер по порядку в текущий день (01,02, итд)

И такой же делаем для обратной зоны

Следует заметить что bind будет сам позже дописывать эти файлы по своему усмотрению

Далее – возвращаемся в /etc/dhcp/dhcpd.conf и либо добавляем include на файл
include "/etc/bind/rndc.key";
либо копируем содержимое rndc.key и вставляем в верх конфигурации dhcpd.conf

Я выбрал второй вариант – так как для include необходимо править права на доступ к файлу и т д – а мне лень

И добавляем наши зоны

На этом можно закончить настройку – при перезапуске proxmox все должно работать – если все ок то в /var/cache/bind появятся .jnl файлы наших зон

И наши машины будут пинговатся по hostname

Так же надо упомянуть что при изменении hostname соотвествующее изменение нужно сделать в /etc/hosts

Далее нам нужно настроить наш bastion чтобы он работал как reverse-proxy и проксировал запросы из внешней сети на нужные нам машины

В своем случае я хочу чтобы reverse-proxy так же проксировал также и доступ на UI proxmox

В качестве reverse-proxy я буду использовать nginx

Устанавливаем nginx (apt install nginx)

После установки создается папка /etc/nginx/ с файлами конфигурации

Основная конфигурация находится в /etc/nginx/nginx.conf – она так же содержит строку include /etc/nginx/sites-enabled/\* которая ссылается на дополнительные файлы с описанием конфигурации конкретных серверов – я буду придерживаться такой же парадигмы и буду создавать сервера в отдельных файлах в этой папке

В основном файле я добавил единственную строчку - include /etc/nginx/proxy.conf; которая указывает на файл параметров конфигурации проксирования

Для начала сделаем сервер для редиректа http → https

Далее сделаем сервер для проксирования запросов к UI proxmox

Следует заметить – я заранее выписал сертификаты через zerossl.com и перенес их на bastion через scp (в дальнейшем можно упростить и автоматизировать эти шаги сделав джобу)
Так же – для нормальной работоспособности UI необходимо прокинуть заголовок Upgrade (используется для websocket соединений)

Настоятельно рекомендуется сделать Snapshot машин на этом моменте

Далее – подготовим template для наших машин k8s

Для начала нужно учесть что для k8s пашин необходимо иметь минимум 2Гб RAM, 2CPU и все машины должны иметь подключение к интернету

Я буду устанавливать k8s через kubeadm – все установки буду делать на хосте node1 – далее сконвертирую его в template и буду клонировать его, изменяя hostname и сопутствующие переменные, необходимые для работы k8s (например MAC и product_uuid), для создания controller и других node

Обновим apt-get: apt-get update
Уставновим containerd (можно выбрать и другую container-runtime – гугл в помощь):
apt install containerd
После установки у нас появился каталог /etc/containerd/ в котором лежит файл config.toml

Для начала нужно поменять его на предлагаемый default конфигурацией:
sudo containerd config default → и копируем вывод в наш config.toml файл

Дефолтный конфиг нас устраивает – но нужно заменить 1 строчку

SystemCgroup – по умолчанию = false – меняем на true

sudo systemctl enable containerd

Теперь нужно отключить swap – так как k8s требует чтобы он отсутствовал (на текущий момент это так – но в документации упонимается что альфа версии k8s можно настроить работу со swap – как будет в дальнейшем – хз)

swapoff --all
nano /etc/fstab – комментируем строчку упаминающую swap
если такой строчки нет – то нет и swap – тогда шаги по его отключению можно пропустить
nano /etc/initramfs-tools/conf.d/resume – комментируем строчку с GUID из предыдущего файла

Теперь нам нужно расширить наш системный диск освобожденным от swap местом

Найдем как называется наш диск: lsblk
И будем использовать утилиту fdisk /dev/sda (в моем случае диск называется sda)

Command (m for help): p
Device Boot Start End Sectors Size Id Type
/dev/sda1 2048 22527 20480 10M ef EFI (FAT-12/16/32)
/dev/sda2 22528 186367 163840 80M 83 Linux
/dev/sda3 186368 204799 18432 9M 82 Linux swap / Solaris

# Delete the swap partition

Command (m for help): d
Partition number (1-3, default 3): 3
Partition 3 has been deleted.

# Delete the system partition

Command (m for help): d
Partition number (1,2, default 2): 2
Partition 2 has been deleted.

# Create a new system partition starting the same as the old one but ending a bit farther, at the end of the (now deleted) swap partition

Command (m for help): n
Partition type
p primary (1 primary, 0 extended, 3 free)
e extended (container for logical partitions)
Select (default p):

Using default response p.
Partition number (2-4, default 2):

# Enter the same start sector as the old part.

First sector (22528-204799, default 22528): 22528

# Enter the end sector of the old swap partition

Last sector, +sectors or +size{K,M,G,T,P} (22528-204799, default 204799): 204799

Created a new partition 2 of type 'Linux' and of size 89 MiB.

# Save the changes

Command (m for help): w
The partition table has been altered.

Далее используем resize2fs /dev/sda2

Теперь нам нужно включить прокидывание пакетов ip4 – так как k8s так же это требует

захидим в /etc/sysctl.conf и раскомментируем строчку

Далее создаем файл /etc/modules-load.d/k8s.conf
и прописывем в нем строчку br_netfilter

Прокидывание пакетов нужно для функционирования k8s кластера

Далее нам нужно поставить k8s репозитории

sudo apt install gpg

Поставим gpg ключ: sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

Устанавливаем репозиторий: echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list

В приведенной выше команде используется версия k8s – 1.29 которая последняя на текущий момент.

sudo apt update

Теперь установим пакеты для K8S

sudo apt install kubelet kubeadm kubectl -y

kubeadm – утилита для установки кластера а так же присрединения новых нод к кластеру
kubectl – утилита для взаимодействия с кластером
kubelet – агент для взаимодействия между нодами

После установки этих пакетов мы завершием создание template для нашего k8s – остальная конфигурация будет отличатся для controller ноды и worker ноды

Template можно сдлеать по разному – но я просто конвертировал VM в template и склонировал его 2 раза – изменив /etc/hostname в controller ноде

Настроим controller1.dev.k8s.internal -
sudo kubeadm init --control-plane-endpoint=controller1.dev.k8s.internal --node-name controller1 –pod-network-cidr=10.244.0.0/16

После выполнения будет выведены команды для присоединения остальных controller-node и worker-node – их нужно сохранить

Так же нам будет предложено выполнить

mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Чтобы управлять кластером не с root пользователя – нужно их выполнить

Если прописать kubectl get pods –all-namespaces можно увидеть что у нас есть несколько coredns подов которые не стартанули

Для того чтобы все работало на нужно установить flannel overlay network
kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml

Через минуту – все должно работать

Мне показалось что 2 CPU и 2Гб RAM мало для controller ноды – поэтому я добавил еще столько же

Далее нам нужно подключить наши worker ноды(ноду) к контроллеру

Для этого подключимся к нашей node1.dev.k8s.internal и пропишем комманду которую нам вывела init команда в предыдущем шаге (исполнять ее нужно под sudo)

Так как предыдущая команда содержит токен соединения – он не постоянный – и у него есть expiration time – так что если токен истек можно создать новый командой (на controller ноде)
kubeadm token create –print-join-command (для присоединения controller ноды добавить в вывод –control-plane)

Я добавил еще одну worker ноду – просто на всякий случай

Далее – нам уже можно развернуть наши сервисы в k8s

Для теста – я разверну kubernetes-dashboard
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml

Получить текущий yaml конфиг сервиса можно так - kubectl get <type> <name> -n <namespace> --output yaml

Для того чтобы к dashboard был доступ извне можно сделать разными способами – я сделаю ingress

Но для начала – для упрощения дальнейшей конфигурации – установим helm

helm можно установить на локальную машину – но для его работы нужно чтобы локальная машина имела доступ к кластеру через kebectl – прокидывать подключение через reverse-proxy до него сложно и небезопасно (у меня лично была проблема с сертификатами при попытке прокинуть tcp через stream на nginx и идея прокидывать подключение копируя сертификаты доступа в nginx для меня показалось неособо хорошей идеей)
Поэтому установим helm на bastion машину и будем работать через нее

curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
sudo apt-get install apt-transport-https --yes
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
sudo apt-get update
sudo apt-get install helm
так же установим kubectl (так же как указано выше)

чтобы поключится к k8s с bastion мы будем использовать kubeconfig – заходим на наш controller1.dev.k8s.internal и копируем ~/.kube/config в такую же папку на bastion – после этого мы можем обращатся к k8s кластеру через наш kubect

Nginx ingress можно установить так же через helm – НО установка через helm будет расчитывать что в системе есть LoadBalancer который будет выдавать внешние IP – на нашей системе таких нет

Для того чтобы ingress-controller приобрел IP для своего LoadBalancer нам нужно так же поставить metalLB – которому дать диапазон IP для выдачи, но следует заметить что (согласно документации) metalLB будет в любом случае ботленеком в системе так как он выделает в кластере 1 ноду лидера и все запросы к сервисам типа LoadBalancer будут проксироватся через нее

Поэтому я решил пойти по документации nginx для bare-metal кластера
https://github.com/kubernetes/ingress-nginx/blob/main/docs/deploy/index.md#bare-metal
https://github.com/kubernetes/ingress-nginx/blob/main/docs/deploy/baremetal.md

После установки контроллера поставим правила роутинга (правило роутинга (ingress) должно быть в том же namespace что и сервис на который роутится)

configuration-snippet выключены по дефолту – для того чтобы их включить нужен ресурс ConfigMap
Стоит обратить внимание что nginx будет искать ConfigMap по имени указанному в конфигурации deployment
kubectl describe deploy ingress-nginx-controller -n ingress-nginx

В моем случае у меня были проблемы с ssl при попытке применить эту конфигурацию (error when creating "kubernetes-dashboard-ingress": Internal error occurred: failed calling webhook "validate.nginx.ingress.kubernetes.io": failed to call webhook: Post "https://ingress-nginx-controller-admission.ingress-nginx.svc:443/networking/v1/ingresses?timeout=10s": tls: failed to verify certificate: x509: certificate signed by unknown authority)

я решил их так -
CA=$(kubectl -n ingress-nginx get secret ingress-nginx-admission -o jsonpath='{.data.ca}')
kubectl patch validatingwebhookconfigurations ingress-nginx-admission --type='json' -p='[{"op": "add", "path": "/webhooks/0/clientConfig/caBundle", "value":"'$CA'"}]'

Начиная с версии 1.18 при создании ingress также необъодимо указать ingressClassName

Но для начала такой ingressClass надо создать

Далее – добавим правила роутинга на ingress в наш внешний nginx reverse-proxy
Ссылаться можно на любую ноду – это не имеет значения

31291 – это NodePort ingress контроллера

Сейчас у kubernetes-dashboard указан base-path как “/” то есть при переходе по нашему URL (у меня это node1.dev.k8s.internal/dashboard) буедт получаться фронтовая часть дашборды – но так как в ней указан base-path “/” - все остальные запросы от фронта будут идти на node1.dev.k8s.internal/ а не на node1.dev.k8s.internal/dashboard

Поэтому на внешнем nginx я прописал отдельный роут для дашборды

Для того чтобы зайти на dashboard нужен токен или kubeconfig – я буду использовать токен – для этого создадим пользователя

Сделаем его админом

И выпишем ему токен

Сделаный токен можно посмотреть выполнив команду -
kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath={".data.token"} | base64 -d

Далее – нам нужно развернуть Keycloak – как наше решение для аутентификации пользователей
Vault – как хранилище секретов которые мы будем использовать как в приложениях так и в k8s
Nexus – как хранилище артефактов (helm charts, nuget, image)

А так же развернуть пару агентов Gitlab для сборки и разворачивания наших приложений

Для начала установим Vault – лучший способ это сделать – helm
Вот только hashicorp достаточно пидарасы – поэтому заблокировали доступ для россии к своим ресурсам – поэтому скачать чарт разворачивания vault можно только через vpn или proxy

helm repo add hashicorp https://helm.releases.hashicorp.com

Далее я взял дефолтный values.yaml и немного его изменил – включил ui, injector и сконфигурировал хранилище

Для того чтобы мы могли давать место подам под наши данные – нам нужен регулятор подобной фанкциональности – для этого в cloud решениях k8s существуют свои provisioner’ы но мы будем использовать NFS – для этого хорошо бы расширить диск на наших нодах proxmox

Для начала – нам нужен NFS сервер – создаем его deployment

NFS можно так же развернуть через helm - https://github.com/kubernetes-csi/csi-driver-nfs?tab=readme-ov-file#readme

Далее делаем сервис -

Далее нам нужен драйвер для этого всего

helm repo add csi-driver-nfs https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/charts
helm install csi-driver-nfs csi-driver-nfs/csi-driver-nfs --namespace kube-system --version v4.6.0

Так же нам нужно создать StorageClass для нашего хранилища

Обращаю внимание на параметр server – это имя нашего сервиса которого мы развернули выше в виде {serviceName}.{namespace}.svc.cluster.local

После этого устанавливаем vault - helm install vault hashicorp/vault –values values.yaml

Указывая наш values.yaml

В values я изменил соответственно эти настройки передав значение нашего storageClass

После этого под Vault должен быть в статусе Running но Ready будет 0/1

Если что то идет не так пользуемся командой kubectl describe pvc data-vault-0 -n vault
Там в графе Message будет возможная ошибка (если она произошла в момент запроса volume у нашего nfs)

Теперь нам нужно проинициализировать Vault

kubectl exec -n vault -ti vault-0 -- vault operator init

Данная команда напишет нам ключи для разпечанования хранилища – нам нужно распечатать его хотябы 3 ключами

kubectl exec -n vault -ti vault-0 -- vault operator unseal {KEY}

Повторяем эту команду 3 раза с 3 разными ключами – после этого под Vault будет Ready

Следует заметить что Hashicorp – пидарасы и разворачиватся под не будет без VPN – так что это надо иметь в виду – будет ошибка вида “Cannot find image hashicorp/vault ….” посмотреть можно через логи Satefulset’а

При перезагрузке ноды на которой расположен vault операцию unseal нужно будет повторить

Для пущей секьюрности я изменил конфиги nginx и ingress для того чтобы до сервисов типа /Vault /Dashboard и т д был бы доступ только из локальной сети

Тут следует пояснить -
у k8s-dashboard все урлы – относительные строчка rewrite /dashboard($) dashboard/ permanent;
будет редиректить например урл https://example.com/k8s/dashboard на https://example.com/k8s/dashboard/
Этот последний слеш для нас очень важен так как по всех ссылках на dashboard относительные URL обращения к сервису (то есть например “test/somescript.js – без / вначале) поэтому фронт будет для всех урлов обращатся на https://example.com/k8s/dashboard/something а если этого слеша не будет – то обращение будет на https://example.com/k8s/something

Но к нашим сервисам должен быть доступ из интернета – так что все что будет идти на /k8s/service будет проксироватся на ingress ТО ЕСТЬ
Если у нас есть сервис “Test” у которого есть роут /Issue/{IssueId}
то чтобы на него попасть нужно сделать запрос на dev.pents.online/k8s/service/Test/Issue/111

Так же нам нужно сделать наш Ingress для vault UI
Нужно учитывать что Vault (зачем то) имеет прямые урлы в своем сервисе а так же захардкоженый редирект с /Something на /ui

И на этом месте – после 12 часов попыток хоть как то проксировать этот ебаный vault на нужный мне url – я понял что альтернативное решение – не такое плохое
Я пробовал все -

И сделал вывод что backend сервис vault расчитывает что он как последняя скотина всегда будет на / и ничего его не ебет и он как последняя скотина всегда редиректит на /ui а в js бандлах этот /ui В НЕКОТОРЫХ СЛУЧАЯХ подменяется на пустую строку – и без изменения base_url самого бекенда ничего работать должным образом не будет

Даже если мне бы и чудом каким то удалось бы достичь цели – подобное решение точно не best practice и жить с ним не очень хорошо

infisical !

helm repo add infisical 'https://dl.cloudsmith.io/public/infisical/helm-charts/helm/charts/'

так же скачиваем дефолтный values.yaml и меняем под себя

helm upgrade --install \
 -n infisical --create-namespace \
 -f custom-values.yaml \
 infisical infisical/infisical
In the meanwhile – пропатчим наш StorageClass чтобы он был дефолтным

Ну либо придется заполнять StorageClass внутри values.yaml
Так как других storageClass у нас нет – этот будет дефолтным

Если при разворачивании появляется ошибка /opt/bitnami/scripts/libos.sh: line 344: 61 Illegal instruction "$@" > /dev/null 2>&1

Вот ответ - https://github.com/bitnami/charts/issues/12834?ref=tech2rue.sussudio.ovh

Вкратце – для работы mongoDB нужно чтобы проц поддерживал AVX инструкцию

В моем случае в proxmox был выбран CPU из пака QEMU процов – который не поддерживает эту инструкцию – я просто поменл процы на нодах на “host”

Список поддерживаемых инструкций можно посмотреть через cat /proc/cpuinfo

После того как все перезагрузилось – все ок

И снова все сносим – потому что это не тот (не последний) чарт ! (FUCK!!!!!)

Полная инструкция здесь - https://infisical.com/docs/self-hosting/deployment-options/kubernetes-helm

helm repo add infisical-helm-charts 'https://dl.cloudsmith.io/public/infisical/helm-charts/helm/charts/'

helm repo update

Берем конфиг отсюда - https://raw.githubusercontent.com/Infisical/infisical/main/helm-charts/infisical-standalone-postgres/values.yaml

И правим под наши нужды

Здесь можно посмотреть последние билды - https://hub.docker.com/r/infisical/infisical/tags

Создаем секрет infisical-secrets

И устанавливаем через хелм helm upgrade --install infisical infisical-helm-charts/infisical-standalone --values

А на этом месте я понял что роутить по сабдомену намного проще чем по запросу

Новая схема враимодействия:

Есть публичные URL сервисов развернутых внутри кубера: \*.service.k8s.dev.pents.online
Есть приватные URL приложений к которым должен быть доступ только из локальной сети:
dashboard.k8s.dev.pents.online – k8s Dashboard
infisical.k8s.dev.pents.online – хранилище секретов
keycloak.k8s.dev.pents.online – сервис авторизации (блочить можно не все URL но так как все сервисы у нас будут во внутренней сети кубера – запросы к сервису авторизации извне нам не нужны, пока что)
ve.pents.online – это наш proxmox UI

Для этого во первых – обнулим наши ingress – то есть сделаем так чтобы они роутили с / на / и так же имели одноименный hostname (те – dashboard-ingress будет иметь hostname dashboard.k8s.dev.pents.online)

Во вторых исправим наш внешний nginx – для того чтобы разграничить доступы нам нужно заменить наш 443 ssl http proxy на L2 TCP proxy то есть воспользоваться stream

\*Стоит заметить что stream равен http так что в конфигурации nginx он должен находится на одном уровне с http

Чтобы понять куда роутить TCP трафик мы воспользуемся ssl_preread_server_name которая прочитает название хоста для нас и в зависимости от названия мы перенаправим запрос на внутренний http proxy

Пример – в данном случае это роутинг на proxmox UI

Здесь стоит пояснить – чтобы на внутреннем http proxy мы увидели настоящий IP клиента мы должны включить диррективу proxy_protocol в верхнем TCP proxy и аннотировать http proxy такой же диррективой

Так же поясню – дирректива proxy_protocol включает общение с сервером через PROXY protocol (L4 TCP) и имеет две вариации – если она указана в listen {PORT} proxy_protocol то сервер будет ПРИНИМАТЬ proxy_protocol – если она указана в proxy_protocol on; - то сервер будет ПОСЫЛАТЬ proxy_protocol (можно указать сразу обе диррективы)

Так же – задать диапазон “доверенных” адресов от которых можно принимать “реальный” IP клиента (set_real_ip_from)
И после этого на основе уже реального IP мы смотрим блокировать запрос или нет
Так как нас интересуют только адреса локальной сети (у меня это 192.168.50.Х) то мы конфигурирем это через allow – deny (эти 2 директивы работают переписывая друг друга причем deny имеет более высокий приоритет – поэтому сначала пишем allow – а потом deny)

\*Можно заметить что в списке сервисов на скринах появился nginxui и появился сертификат в конфигурации http proxy который роутит на proxmox
NxinxUI (https://nginxui.com) я поставил для упрощения просмотра логов и конфигурации
Сертификаты я выписал через Let’sEncrypt используя certbot (и только потом понял что nginxui может это сделать автоматически, и так же автоматически будет их обновлять)
Как это сделать можно найти по первой ссылке в гугле

Чтобы нмы могли создать map на несколько адресов разом (то есть через \*) нужно в конфигурацию map добавить специальный параметр hostname;

Теперь все адреса .k8s.dev…. Будут находится и роутится – без этого nginx не будет находить upstream

Я еще раз немного изменил роутинг – чтобы разделить доступы на сервисы и на инфраструктуру

И сделал соответственно 2 Http proxy которые слушают на 5100 и 5200 соответственно
где сами прокси идентичны за исключением того что в 1 есть allow-deny а в другом нет

Делая http прокси – нужно учитывать что у ingress есть 2 порта – 1 для http другой для https

Делая ingress для k8s dashboard следует учитывать что сам бекенд дашборды ожидает HTTPS соединение – но ingress разворачиват https в http и отправляет на заданный порт из за чего могут быть ошибки при доступе – решается просто
annotations:
nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
nginx.ingress.kubernetes.io/ssl-passthrough: "true"

Теперь для того чтобы использовать НАКАНЕЦ ТО поставленый infisical внутри k8s и подставлять секреты из infisical в конфиги сервиса – нужно поставить еще infisical-secrets-agent

helm install secrets-operator infisical-helm-charts/secrets-operator -n infisical

Теперь мы можем сделать секрет типа InfisicalSecret который будт синхронизироватся с нашим хранилищем секретов и автоматически обновлять свое значение

Для начала – перейдем на Dashboard infisical в наш проект и создадим ServiceToken для нашего проекта

И создадим секрет в k8s хранящий наш ServiceToken
kubectl create secret generic {TokenName} -n {namespace} –from-literal=infisicalToken="{TOKEN}"

Так же сделаем GlobalConfig

NameSpace и Name здесь должны быть именно такими чтобы конфиг подтягивался куда нужно
HostAPI – в данном случае это путь до сервиса в k8s (будет работать только если infisical установлен в этом же кластере)

И делаем наш secret

В данном случае это секрет для Keycloak который берет секреты из

Чтобы проверить что все ок можно посмотреть
kubectl describe infisicalsecret infisical-keycloak-admin-secret -n keycloak

Там должна быть строчка Message: Infisical controller has started syncing your secrets
Ну либо ошибка которую следует разобрать

А в указаном в InfisicalSecret неймспейсе должен появится указаный Secret

Далее – использовать наш секрет в конфигурации сервиса можно так

Далее чтобы упростить работу с сертификатами для наших сервисов мы развернум certificate manager

helm repo add jetstack https://charts.jetstack.io

Поидее нас устраивают дефолтные значения – поэтому values.yaml нам не нужен – но при желании посмотреть можно здесь - https://artifacthub.io/packages/helm/cert-manager/cert-manager?modal=values

Устанавливаем CustomResourceDefenition – нужные ресурсы для работы с cert-manager (можно их так же установить через helm – но в таком случае при обновлении чарта – все ресурсы сбросятся и все сертификаты пропадут)

kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.4/cert-manager.crds.yaml

helm install \ cert-manager jetstack/cert-manager \ --namespace cert-manager \ --create-namespace \ --version v1.14.4

Учитывайте что в командах выше указана версия 1.14.4 – версии могут отличатся в зависимости от времени

Теперь создадим Issuer – можно создавать Issuer на каждый namespace – но в моем случае я решил использовать один глобальный ClusterIssuer

Теперь можно обновить наш (например) k8s-dashboard чтобы он брал сертификат из cert-manager

Естественно перед этим нужно снять allow-deny с нашего внешнего nginx чтобы запрос на /.well-known смог пройти

По дефолту – если выписать сертификат не получилось cert-manager будет пытаться повторить попытку через час- потом через 2 – потом через 3 и т д
ЧТобы триггернуть попытку вручную нужно установить cmctl
И выполнить команду cmctl renew {certificateName} -n {certificateNamespace}

Так же я немного изменил конфигурацию внешнего nginx чтобы он роутил все \*.k8s.dev.pents.online запросы напрямую в k8s без дополнительной прослойки ввиде L7 Http proxy

Я отключил proxy_protocol для внутренних соединений – я думаю эту схему стоит пояснить

Теперь хотелось бы закрыть все внешние соединения для нашего bastion хоста за исключением порта 443
Для этого установим iptables
Изначально конфигурация у нас позволяет все

Создадим файл конфигурации для iptables – sudo nano /etc/iptables.router

Далее – я буду использовать следущую конфигурацию – учитывайте что копируя ее нужно как минимум поменять интерфейсы под свои нужды
#!/bin/bash

# Объявление переменных

export FW="iptables"

# Интерфейс, подключенный к локальной сети

LAN="enp0s19"

# Интерфейс, подключенный к Интернету

WAN="enp0s18"

# Диапазон адресов локальной сети

LAN_NET="192.168.50.0/24"

# Очистка всех цепочек и удаление правил

$FW -F
$FW -F -t nat
$FW -F -t mangle
$FW -X
$FW -t nat -X
$FW -t mangle -X

# Разрешить входящий трафик к интерфейсу локальной петли,

# это необходимо для корректной работы некоторых сервисов

$FW -A INPUT -i lo -j ACCEPT

# Разрешить на внутреннем интерфейсе весь трафик из локальной сети

$FW -A INPUT -i $LAN -s $LAN_NET -j ACCEPT

# Позволяем два, наиболее безопасных типа пинга

$FW -A INPUT -p icmp --icmp-type 0 -j ACCEPT
$FW -A INPUT -p icmp --icmp-type 8 -j ACCEPT

# Разрешить входящие соединения, которые были разрешены в рамках других соединений

$FW -A INPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT

# Разрешить перенаправление трафика с внутренней сети наружу, как для новых, # так и уже имеющихся соединений в системе

$FW -A FORWARD -i $LAN -o $WAN -m conntrack --ctstate NEW,RELATED,ESTABLISHED -j ACCEPT

# Разрешить перенаправление только тех пакетов с внешней сети внутрь,

# которые уже являются частью имеющихся соединений

$FW -A FORWARD -i $WAN -o $LAN -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT

# Выполняем трансляцию сетевых адресов, принадлежащих локальной сети

$FW -t nat -A POSTROUTING -o $WAN -s $LAN_NET -j MASQUERADE

# Открываем порт 443 для входящих соединений

$FW -A INPUT -p tcp --dport 443 -j ACCEPT

# Открываем порт 80 для входящих соединений

$FW -A INPUT -p tcp --dport 80 -j ACCEPT

# Открываем порт 22 для входящих локальных соединений (ssh)

$FW -A INPUT -s 192.168.50.0/24 -p tcp --dport 22 -j ACCEPT

# Устанавливаем политики по умолчанию:

# Входящий трафик — сбрасывать; Проходящий трафик — сбрасывать;

# Исходящий — разрешать

$FW -P INPUT DROP
$FW -P FORWARD DROP
$FW -P OUTPUT ACCEPT

---

Делаем файл исполняемым – sudo chmod +x /etc/iptables.router
Запускаем его – sudo /etc/iptables.router
Чтобы после перезагрузки маршрутизация заработала автоматически — нужно добавить в файл /etc/sysctl.conf строку или привести уже имеющуюся к виду:
net.ipv4.ip_forward = 1
sudo apt install iptables-persistent

Сохраняем конфиг чтобы после перезагрузки все было так как мы поставили - sudo iptables-save

Последнюю команду нужно выполнять под root (sudo su)

Теперь у нас запрещены все соединения кроме порта 443 и 80 на которых у нас основной tcp proxy и https redirect

Но – у нас все еще остается 1 проблема – если вдруг у нас пропадает интернет соединение – по какой либо причине на стороне провайдера – нам нужно перезагрузить сервер чтобы он вновь подключился к сети (причем весь сервер).
В идеале нам нужно проверять есть ли у нас интернет соединение и перезагружать network если его нет – через какой нибудь промежуток времени

Для этого зайдем на proxmox хост и создадим там cron джобу
sudo nano /etc/cron.custom/network-restart
#!/bin/bash

ping -c 1 8.8.8.8
received=$?
echo $received
if [[$received -ne 0]] ; then
service network-manager restart
fi

sudo chmod +x /etc/cron.custom/network-restart

sudo crontab -e

_/5 _ \* \* \* /etc/cron.custom/network-restart

Данная джоба пингует гугл – и если пинг не прошел – перезагружает network – cron выражением мы запускаем ее каждые 5 минут

Теперь развернем наш Keycloak

Для начала – развернем наш ingress для кейклока – чтобы он сразу выписал нам наш сертификат

Мы уже создали и заполнили наш InfisicalSecret выше – когда устанавливали и проверяли Infisical-secret-agent

Создадим DB для нашего keycloak чтобы сохранять персистентность

И сервис для нее

Сервис для нашего keycloak

И сам keycloak-statefulset

Учитывайте – чтобы Service нашел под на которой ему ссылатся – в поде нужно прописать labels:
app: {YOUR_LABEL}
И тот же лейбл в сервисе
selector:
app: keycloak

Так же лучше сверится с переменными в официальной конфигурации https://www.keycloak.org/server/all-config?q=db
Так как от релиза к релизу – они иногда меняются

Теперь поставим Nexus

Полная инструкция здесь - https://artifacthub.io/packages/helm/stevehipwell/nexus3

Принцип тот же – скачиваем values.yaml и модифицируем под себя – далее устанавливаем репозиторий – потом чарт с указанием нашего values.yaml
Для себя я создал InfisicalSecret который будет хранить пароль админа и выключил anonimus соединения – включил ingress с указанием хоста как nexus.k8s.dev.pents.online и указал сертификат – который сделан через cert-manager
Стоит заметить что название ключа для secret – case-sensitive – так что это стоит учитывать создавая ключ в infisical

В nexus можно настроить SSO через LDAP используя наш Keycloak или сделать proxy поверх соединения к nexus чтобы proxy выполняло авторизацию в Keycloak по OpenID – но для этого надо ставить отдельный LDAP сервер или заморачиватся с Third-party решениями для подобного OpenId proxy либо делать его самому – а мне лень – так что создавать пользователей буду напрямую в nexus (но возможно позже вернусь к этой проблеме)

Далее – создадим репозиторий для наших helm-charts

Заходим в конфигурацию Repositories → CreateRepository → Выбираем тип helm (hosted) задаем имя
Переходим в настройки ролей и создаем роли для read-only и helm-admin
Переходим в настройки пользователей и создаем 2 пользователей для read-only и helm-admin

Добавляем наш репозиторий чартов в helm
helm repo add nexus https://helm-read-only:{PASSWORD}@nexus.k8s.dev.pents.online/repository/helm-charts/
В данном случае мой пользователь helm-read-only

Тоже самое повторим для наших container-image
Указываем желаемый HTTP порт при создании – HTTPS нам не нужлн так как обращатся к репозиторию мы будем изнутри k8s
Создаем отдельный blob-storage для наших image
А так же 2 пользователей для Read и Write

Так же для docker-repository нам нужно дать отдельный URL (у меня это будет docker.nexus.k8s.dev.pents.online) немного изменить values.yaml и добавить аннотацию к ingress чтобы небыло ошибки 413 Request too large при загрузке образов

Теперь – нам нужны агенты для билда наших container-image и разворачивания запушенного в gitlab кода на нужный нам сервис (про CI/CD будет дальше)

Заходим в Gilab → Build → Runners и создаем runner для Linux после создания будет показан –token который нам нужно скопировать и сохранить (по этому токену runner будет подключатся к gitlab)

Далее берем наш values.yaml (https://gitlab.com/gitlab-org/charts/gitlab-runner/blob/main/values.yaml) и модифицируем под себя – главное вставить скопированный выше токен в поле runnerToken
Я сделал токен через infisicalSecret – для этого в values.yaml есть отдельное поле (но помимо runnerToken необходимо так же указать runnerRegistrationToken как пустое значение – как говорит документация – это backwardsCompatbility)

\*тут следует пояснить – у Gitlab есть agent и runner – в нашем случае нами нужен именно runner так как логика деплоя, сборки, обновления и т д – будет у нас на сервере управлятся нашими (вдальнейшем сделанными) helm-charts
agent – нужен для унификации взаимодействия gitlab с k8s – он напрямую подключается к кластеру и используя свои методы деплоя и обновления – сканирует запушеный код на предмет .gitlab-ci и обновляет соответственно поды в кластере – это удобно если весь ci/cd процесс отдается под управление gitlab (container-registry, charts, images и т д) но в нашем случае мы хотим добится большей доли управления в этом процессе чтобы в дальнейшем не зависеть от конкретной логики агента сборки (например если мы хотим переехать в github или еще куда либо)

Полная документация по установке через helm - https://docs.gitlab.com/runner/install/kubernetes.html

Добавляем репозиторий с gitlab-runner
helm repo add gitlab https://charts.gitlab.io

Применяем helm-chart
helm install --namespace gitlab --create-namespace gitlab-runner --values values.yaml gitlab/gitlab-runner

Для использоватия helm внутри наших job мы будем использовать
image:
name: alpine/helm:latest
entrypoint: [""]

Стоит учесть что если использовать gitlab.com как репозиторий – то необходимо указать в values.yaml поле gitlabUrl именно как https://gitlab.com именно https иначе не будет работать

Так же gitlab-runner должен иметь указанного executor: Kubernetes и созданный ServiceAccount (RBAC)

Если все сделано правильно – в гитлабе появится челеный значек

Следующий “milestone” – это сделать CI/CD – которое сможет

1. Собрать наш проект
2. Вызвать тесты и проконтролировать output
3. Создать образ контейнера для разворачивания (учитывая желаемые переменные)
4. Развернуть наш контейнер на k8s и создать автоматически Ingress с нужным хостом

Все это должно быть сделано через helm с кастомными чартами – при редеплое или обновлении сервиса – должена создаватся новая версия того же релиза helm и предыдущие значения должны сохранятся с retension = 10

В моем случае – я хочу разворачивать приложение написаное на Rust – поэтому буду использовать такой Dockerfile

Тут к сожалению мне пришлось немного попотеть чтобы вывести именно то что нужно – я разрабатываю на Mac причем на M2 чипе – поэтому в моем случае просто так закомпилить этот Docker-образ не получится – но на сборке через runner все ок, насколько я понял – у Actix который я использую в качестве web фреймворка есть какие то зависимости на либы которые неособо хотят компилится под arm – так что – имеем что имеем.
Единственный образ который у меня заработал для билда приложения – это docker.io/blackdex/rust-musl:x86_64-musl
Все остальные – не работают

Тут следает пояснить что вообще происходит -
Сначала берется образ-основа на которой будет происходить сборка приложения
Далее – задается таргет musl – это нужно для сборки static binary
Обновляются необходимые либы для сборки
Копируются файлы из текущей директории(там где Dockerfile) в /src на стороне контейнера
WORKDIR = cd
Запускаем build приложения – указываем дополнительно –locked – чтобы во время билда небыло подтянуто левых либ – а использовались только наши – чте что указаны в .locked файле, так же указываем –target который задали так же выше
После билда копируем appsettings.json который необходим в моем случае

Далее – выбираем систему на которой у нас будет хостится наше сбилженое приложение – в моем случае это scratch (пустой образ системы)
Копируем туда наш бинарник и наш appsettings.json
Задаем порт на котором будет работать приложение (в моем случае переменная PORT используется внутри приложения для определения порта) и открываем его для внешних соединений
И задаем точку входа – то есть наше приложение

Вернемся в наш gitlab-runner и немного изменим конфиг

Здесь я добавил лимиты на сборку – так как если их небудет runner будет занимать буквально все что есть на выделенной машине
А так же – задал путь внутри конкретных подов runner по которому будут доступны секреты из gitlab-secret (это наш конфиг авторизации docker-registry и helm-registry)

Сделаем джобу для CI/CD для билда данного образа в нашем pipeline

здесь мы берем за основу образ kaniko-executor
Чисто теоритически можно взять и обычный докер или какой нибудь podman – для сборки подойдет любой – но у них есть 1 нюанс – они требудет root доступа (а так же еще каких то прав – гугл в помощь) для того чтобы docker под внутри себя собирал docker образ
Поэтому у меня выбор пал на kaniko – он так же собирает образ по dockerfile и автоматом кидает его в репозиторий
Так как наш конфиг авторизации задан в infisical и прокинут в секрет k8s – этот же секрет мы замонтировали в сборщик – поэтому он нам доступен по пути /gitlab-secret – из него мы берем наш DOCKER_AUTH_CONFIG – который копируем в папку где kaniko будет искать конфиг авторизации (для справки – конфиг авторизации kaniko такой же как в docker или podman – для того чтобы получить его нужно залогинится локально на репозиторий и перейти по ~/.docker/config.json – либо сгенерировать собственными силами – поидее это простая usr:pass зашифрованное в base64)
Kaniko подтягивает конфиг – авторизуется в репозитории – билдит образ – и отправляет его в репозиторий
Следует учитывать что некоторые образы могут быть достаточно увесистыми (>5Gi) что может вызвать проблему на стороне runner (OOM).

В нашем случае из за того что мы используем musl - мы компилим единый файл – который будет содержать все нужные нам библиотеки
Благодоря этому мы уменьшаем во много раз результирующий image

Собственно после прохождения пайплайна мы увидим наш образ в репозитории

Так же замечу – я сделал отдельный роут для внешних соединений для репозитория – но в данном случае я обращался по внутреннему URL (nexus-nexus3.nexus.svc.cluster.local:9876)
Так же – в данном случае мы создаем образ под архитектуру amd64 – нам впринципе этого хватит – но стоит учитывать что можно делать несколько пакетов для разных системм

Стоит учитывать что данная конфигурация поджодит исключительно для запускаемых приложений а не для библиотек – для того чтобы закомпилить и использовать библиотеку на Rust мы будем использовать nexus-repository-cargo плагин так как nexus не поддерживает cargo репозиторий из коробки, а так же отдельную CI конфигурацию которая будет делать cargo push в этот репозиторий – но сделаем мы это позже

Сейчас нам нужно сделать создание наших pod для запушеного приложения

Для этого сдлеаем отдельный репозиторий для наших helm-charts (у меня он будет в группе ci-cd) и отдельную конфигурацию CI/CD для валидации и пуша наших helm-charts в nexus
После чего будем использовать созданный чарт в CI конфигурации сервиса для разворачивания

Определимся что требуется нам от helm-chart

1. Deployment с указанием количества реплик, ресурсов и т д
2. Ingress – котороый будет принимать во внимание название сервиса (которое можно переписать) и будет роутить по дефолту на / - но конфигурацию роутов можно расширить
3. Certificate – который должен выписыватся через cert-manager
4. InfisicalSecret – который будет привязан к каталогу приложения в Infisical и будет задавать переменные окружения подов из Infisical значений
5. Возможность конфигурировать отдельные части – например выключить сертификат и ingress – если сервис подразумевается только для внутренних соединений

Создаем отдельный репозиторий и выполняем helm create {Chart-Name} – данная команда создаст папку с дефолтным шаблоном helm-chart который нам нужно будет изменить под наши требования

Для того чтобы полить наши образы внутри k8s из приватного репозитория – мы будем задавать imagePullSecrets
Который надо будет сначала сделать

kubectl create secret docker-registry secret-tiger-docker \
 --docker-email=tiger@acme.example \
 --docker-username=tiger \
 --docker-password=pass1234 \
 --docker-server=my-registry.example:5000

Сделаем gitlab-ci для helm-charts в отдельном репозитории

Он состоит из 2 стадий
1 – проверяем через lint валиден ли наш репозиторий
2 – тут уже посерьезднее:
так как за основу мы берем alpine – мы должны доставить в него grep который может в perl
также доставляем curl
Далее в нашем Chart.yaml ищем название чарта и версию
Через helm package пакуем наш чарт
Берем из секрета наши креды для пуша в репозиторий
Пушим запаковыный чарт в репозиторий через curl

Сам чарт вставлять в этот документ достаточно накладно – но вообщем он достаточно прост и мало отличается от дефолтного что создается при helm create– разворачивает Deployment, Service, Ingress – для ингреса заданы по умолчанию аннотации для выписывания сертификата через cert-manager а так же предефайненый imagePullSecret

Теперь используя загруженый в репозиторий чарт нужно развернуть приложение

Чтобы gitlab-runner смог запустить под в неймспейсе отличном от его (gitlab в моем случае) – а я хочу чтобы каждое приложение было развернуто в своейм неймспейсе – нужо дать ему права на это:
Я не стал заморачиватся и дал доступ везде

Так же включил
clusterWideAccess: true

и передал этот ServiceAccount в раннеры

Стоит учитывать что при исполнении helm install нельзя комбинировать –set и –values
значения в values будут выигрывать – НО можно создать прям в пайплайне новый файл в текущем каталоге и заполнить его нужными знаениями и передать 2 values.yaml файла друг за другом
Переданный позже yaml будет переписывать предыдущий

Так же helm имеет возможность проверить сгенеренный чарт – я это выделил в отдельный шаг pipeline -
helm install --debug –dry-run

Но стоит учитывать что данная команда выведет все чарты со всеми значениями в stdout – и если есть в значениях пароли – то лучше их замазать

| sed … - удаляет строку содержищуюю “password”

Тут я встретился с очередной НЕЯВНОЙ НИХУЯ проблемой
Сервисы видят друг друга только внутри подов – то есть обратится от одного сервиса к другому по его названию или по .svc.cluster.local можно только изнутри работающего пода (это достигается путем инжекта /etc/resolv.conf на стадии разворачивания)
НО – само разворачивание происходит в контексте конкретной ноды – и ее resolv.conf файла – который (без дополнительных настроек) не видит внутреннюю сеть k8s
Поэтому – обратится в docker-registry можно изнутри script области ci можно – но вызывая тот же helm install – обращение переходит на контекст ноды – и тут все обламывается

Для пула из реподитория я буду использовать внешний URL репозитория
У меня это docker.nexus…..

После изменения пути к репозиторию – все прошло успешно

ИТОГО
У нас есть – настроеный кластер на proxmox, поднятый кластер кубера на 2 нодах,
сервис авторизации который может обрабатывать запросы от сервисов,
репозиторий для docker и helm,
Хранилище секретов
Автовыписка сертификатов
настроеная ci/cd конфигурация для ровного разворачивания сервисов к куберу

В дальнейшем нужно сделать разворачивания БД для сервисов через пайплайн(расширить чарт настройками и расширить ci для разворачивания)
Сделать отдельный cargo-crate репозиторий для хранения библиотек rust
Отдельный pipeline для библиотек rust

В процессе решения проблем вскрылась еще 1 – при оключении света например – в ненужный момент – redis может не оправится от такого (у него может идти запись в его каталог и если она не прошла до конца файл каталога будет битым из за чего связаные с ним сервисы и он сам не поднимутся) для решения этой проблемы нужно создать джобу которая будт проверять этот файл каталога на ошибки и фиксить их если под не поднялся

Примерная логика будет такая

вячм
